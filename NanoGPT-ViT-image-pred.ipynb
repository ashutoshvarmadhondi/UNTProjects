{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb1f193d583848858f29d9677f3156b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eabed16f1de24988b140a65638f53b8a",
              "IPY_MODEL_5e3f1a45db8743ecb6dc6554f4255d51",
              "IPY_MODEL_e1f09ba585ea4d6aa26ab8ef86aeb1e2"
            ],
            "layout": "IPY_MODEL_b6b4ebe44ae04870b240a2f54bea04e6"
          }
        },
        "eabed16f1de24988b140a65638f53b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15099ca3abcf41f0a3404f1d4efc83f5",
            "placeholder": "​",
            "style": "IPY_MODEL_2d69d95cc1024745a224e4b5331b1300",
            "value": "  0%"
          }
        },
        "5e3f1a45db8743ecb6dc6554f4255d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_994ce594a2ec462e93a8b4c0602def59",
            "max": 938,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bd92bb3bcac47efa227896dc5444515",
            "value": 0
          }
        },
        "e1f09ba585ea4d6aa26ab8ef86aeb1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfa0c68a56d4df7a33840ac354d06f0",
            "placeholder": "​",
            "style": "IPY_MODEL_5ede24eb4d164d3f882d27d102a0d839",
            "value": " 0/938 [00:00&lt;?, ?it/s]"
          }
        },
        "b6b4ebe44ae04870b240a2f54bea04e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15099ca3abcf41f0a3404f1d4efc83f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d69d95cc1024745a224e4b5331b1300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "994ce594a2ec462e93a8b4c0602def59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd92bb3bcac47efa227896dc5444515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbfa0c68a56d4df7a33840ac354d06f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ede24eb4d164d3f882d27d102a0d839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Einops** is a Python package that simplifies tensor operations, inspired by Einstein summation. It provides flexible functions for rearranging, reducing, and repeating tensors, making code cleaner and more intuitive"
      ],
      "metadata": {
        "id": "-BpZhAG0W9u7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops vit_pytorch linformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZm4LFTu5R2m",
        "outputId": "04847217-5754-4bdc-aac1-8955cafce6f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: vit_pytorch in /usr/local/lib/python3.10/dist-packages (1.6.5)\n",
            "Requirement already satisfied: linformer in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from vit_pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from vit_pytorch) (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->vit_pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->vit_pytorch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->vit_pytorch) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->vit_pytorch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->vit_pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->vit_pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code checks if CUDA (a parallel computing platform) is available. If it is, it prints “CUDA is available!”; otherwise, it prints “CUDA is not available.”"
      ],
      "metadata": {
        "id": "9JL54-6wXB6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"CUDA is available!\")\n",
        "else:\n",
        "  print(\"CUDA is not available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwzphYZdBS9-",
        "outputId": "ee6d300a-9bd7-451a-ed8c-1d303b710051"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing packages which are required for the project as follows:\n",
        "torch: The core PyTorch library for tensor computation and deep learning.\n",
        "\n",
        "**torch.nn:** Provides tools for defining and training neural networks (such as layers, loss functions, and optimizers).\n",
        "\n",
        "**torch.optim:** Contains various optimization algorithms (e.g., SGD, Adam) for updating model parameters during training.\n",
        "\n",
        "**torchvision.datasets:** A module for loading standard datasets (like MNIST) for computer vision tasks.\n",
        "\n",
        "**torchvision.transforms:** Offers image transformations (e.g., resizing, normalization) for data preprocessing.\n",
        "\n",
        "**datasets:** Part of torchvision, it provides access to popular datasets.\n",
        "\n",
        "**transforms:** Also part of torchvision, it handles data transformations.\n",
        "\n",
        "**DataLoader:** Helps create data iterators for efficient batch processing during training.\n",
        "tqdm: A progress bar library for tracking loops and tasks.\n",
        "\n",
        "**torch.nn.functional:** Contains functional versions of neural network layers (e.g., activation functions).\n",
        "\n",
        "**CrossEntropyLoss:** A common loss function for multi-class classification tasks.\n",
        "\n",
        "**Adam:** An optimization algorithm based on adaptive moment estimation."
      ],
      "metadata": {
        "id": "HjTYqsNzXbL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6OLUtvDLyEyB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from __future__ import print_function\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from linformer import Linformer\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**transforms.Compose:** This function creates a sequence of data transformations. In this case, it combines two transformations: converting the data to a tensor (transforms.ToTensor()) and normalizing it to a range of [-1, 1] (transforms.Normalize()).\n",
        "\n",
        "**trainset and testset:** These are datasets containing MNIST images for training and testing, respectively. The data is normalized using the specified transformation.\n",
        "\n",
        "**trainloader and testloader:** These are data loaders that allow efficient batch processing during training and testing."
      ],
      "metadata": {
        "id": "ns54hy66Y5H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the transform to normalize the data\n",
        "transform = transforms.Compose (\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))] # Normalize to [-1, 1]\n",
        ")\n",
        "\n",
        "# Loading the training and test datasets\n",
        "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wvrFHSByVmX",
        "outputId": "133452d6-423c-40fb-be17-328405953459"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 77933676.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined the categories of images and labelled in a classes"
      ],
      "metadata": {
        "id": "NuxnfbzqeSQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "-OrQG693yVc2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to retrieve a batch of images and their accompanying labels, it initializes an iterator for the training data loader (trainloader). The batch type (images), batch shape (batch size, channels, height, and width), and label shape are then printed."
      ],
      "metadata": {
        "id": "1fd9uFVofuko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rFXVDn1Izenq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**data_iter = iter(train_loader):** Creates an iterator from the train_loader.\n",
        "images is a tensor containing the batch of images.\n",
        "\n",
        "**images.shape** represents the tensor dimensions: batch size (b), channels (c), height (h), and width (w).\n",
        "\n",
        "**labels.shape** indicates the shape of the tensor containing the corresponding labels.\n",
        "\n",
        "Here's a 4-line explanation of the code:\n",
        "\n",
        "1. **Imports:** Gets tools for plotting (matplotlib) and numerical data (numpy).\n",
        "2. **Grabs images and labels:** Fetches a batch of images and their corresponding labels from a training dataset.\n",
        "3. **Inspects an image:** Displays the first image from the batch and prints its shape.\n",
        "4. **Reveals its label:** Prints the class associated with the first image's label."
      ],
      "metadata": {
        "id": "WqrqAaongenL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "print(images.shape)\n",
        "# Show images\n",
        "imshow(images[0]) # Show the 1st image of the batch\n",
        "\n",
        "# Print the label of the first image\n",
        "print('The first Label is:', classes[labels[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "yCaeydGHysHn",
        "outputId": "a15a454c-02f0-4dd5-e07d-39b5824a592a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first Label is: car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code initializes a GPT (Generative Pre-trained Transformer) model specifically tailored for image data processing. It creates a model with 12 layers, each containing 12 attention heads and an embedding size of 768. The model does not include biases and uses a dropout rate of 0.0. Additionally, it is configured with a patch size of 16x16, 3 input channels (RGB), 4 patches per image, and aims to classify images into 10 classes. The model is initialized from scratch and is set to utilize the GPU ('cuda') for computational efficiency during training and inference"
      ],
      "metadata": {
        "id": "aeVvFLRQ0d9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    patch_size: int = 16\n",
        "    channels: int = 3\n",
        "    num_patches: int = 4\n",
        "    num_classes: int = 10\n",
        "    bias: bool = True\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.patch_embeddings = nn.Conv2d(in_channels=config.channels,\n",
        "                                      out_channels=config.n_embd,\n",
        "                                      kernel_size=(config.patch_size, config.patch_size),\n",
        "                                      stride=(config.patch_size, config.patch_size)),\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.n_embd)),\n",
        "        self.pos_embeddings = nn.Parameter(torch.zeros(1, 1 + config.num_patches, config.n_embd)),\n",
        "        self.drop = nn.Dropout(config.dropout),\n",
        "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.num_classes, bias=False)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "\n",
        "            n_params -= self.transformer.pos_embeddings.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Embedding):\n",
        "\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "              module.bias.data.zero_()\n",
        "\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, c, h, w = idx.size()\n",
        "\n",
        "\n",
        "        # Patch embedding\n",
        "        patch_emb = self.patch_embeddings(idx)\n",
        "        patch_emb = patch_emb.flatten(2).transpose(1, 2)\n",
        "\n",
        "        # cls token prepend\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        patch_emb = torch.cat([cls_tokens, patch_emb], dim=1)\n",
        "\n",
        "        # Positional embedding\n",
        "        patch_pos_emb = patch_emb + self.pos_embeddings[:, :x.size(1)]\n",
        "        x = self.drop(patch_pos_emb)\n",
        "        for block in self.h:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}\n",
        "\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "        config_args['bias'] = True\n",
        "\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
        "\n",
        "\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Jj4xfq-MqbGO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It initializes a new GPT model for image processing from scratch with specific configurations: 12 layers, 12 attention heads, 768 embedding dimensions, no biases, no dropout, 16x16 patch size, 3 input channels (RGB), 4 patches per image, and 10 output classes. It prints a message indicating that it's initializing a new model from scratch and creates the model instance using these configurations on the GPU ('cuda')."
      ],
      "metadata": {
        "id": "T5rNBehjiV_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'\n",
        "device = 'cuda'\n",
        "\n",
        "model_args = dict(\n",
        "    n_layer=12,\n",
        "    n_head=12,\n",
        "    n_embd=768,\n",
        "    bias=False,\n",
        "    dropout=0.0,\n",
        "    patch_size=16,\n",
        "    channels=3,\n",
        "    num_patches=4,\n",
        "    num_classes=10\n",
        ")\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv8ItgVoqbD8",
        "outputId": "49e8777b-0e9f-46ca-e2cb-7c1fb38d2966"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " It ensures that the model’s computations are performed on the specified device during training and inference."
      ],
      "metadata": {
        "id": "IBiS-Lyfk79h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa_FILs0qa7U",
        "outputId": "6e0ca192-5028-4013-ace0-966c0e346a98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (ln_f): LayerNorm()\n",
              "  (lm_head): Linear(in_features=768, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function:** nn.CrossEntropyLoss() computes the negative log likelihood loss for multi-class classification tasks.\n",
        "\n",
        "**Optimizer:** optim.Adam(model.parameters(), lr=0.001) initializes the Adam optimizer with a learning rate of 0.001.\n",
        "\n",
        "**Epochs:** epochs = 2 specifies the number of training epochs."
      ],
      "metadata": {
        "id": "34re9SAClIYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# epoch\n",
        "epochs = 2\n"
      ],
      "metadata": {
        "id": "xCaGrHRYqa5P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the accuracy and loss metrics for the training and validation datasets during a training loop that lasts for the chosen number of epochs. For every historical period:\n",
        "\n",
        "It uses tqdm to visualize the progress as it iterates through the training data batches.\n",
        "\n",
        "\n",
        "It delivers the labels and data to the designated device (such as a GPU) so that it can be processed.\n",
        "\n",
        "It computes the loss and predictions (logits) of the model, backpropagates, and uses the optimizer to adjust the model's parameters.\n",
        "\n",
        "It averages over batches to calculate the training accuracy and loss.\n",
        "\n",
        "In a similar manner, it calculates validation accuracy and loss, assesses the model using the validation data, and reports the progress for every epoch."
      ],
      "metadata": {
        "id": "6DMQ7E3qm819"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for data, label in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in test_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(test_loader)\n",
        "            epoch_val_loss += val_loss / len(test_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )"
      ],
      "metadata": {
        "id": "WyxzSFvdm258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "bb1f193d583848858f29d9677f3156b8",
            "eabed16f1de24988b140a65638f53b8a",
            "5e3f1a45db8743ecb6dc6554f4255d51",
            "e1f09ba585ea4d6aa26ab8ef86aeb1e2",
            "b6b4ebe44ae04870b240a2f54bea04e6",
            "15099ca3abcf41f0a3404f1d4efc83f5",
            "2d69d95cc1024745a224e4b5331b1300",
            "994ce594a2ec462e93a8b4c0602def59",
            "7bd92bb3bcac47efa227896dc5444515",
            "cbfa0c68a56d4df7a33840ac354d06f0",
            "5ede24eb4d164d3f882d27d102a0d839"
          ]
        },
        "outputId": "d20ceced-7e22-40bd-9246-0512a8f335ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/938 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb1f193d583848858f29d9677f3156b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'tuple' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bff99341a842>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-402c55385f4f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Patch embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mpatch_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# This is for IMAGE application\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mpatch_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ViT model**"
      ],
      "metadata": {
        "id": "semdsjQ223Bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function pair(t) ensures that the conversion of its input, t, into a tuple. It returns t unchanged if t is already a tuple. If not, it generates a tuple using t twice, guaranteeing that the result is always a tuple—either with a single element repeated or, if t was previously a tuple, with the original contents."
      ],
      "metadata": {
        "id": "JqY_ndk3nCLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)"
      ],
      "metadata": {
        "id": "caga8hzG5j3D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Vision Transformer (ViT) model architecture for image categorization is implemented by the ViT class. Patch embeddings, positional embeddings, the transformer encoder, and a multi-layer perceptron (MLP) head for classification are some of its constituent parts.\n",
        "\n",
        "It adds positional embeddings and transforms picture patches into embeddings. processes the embeddings using a transformer encoder.\n",
        "\n",
        "uses the class token or mean pooling, depending on the pooling type ('cls' or'mean') that has been selected.\n",
        "\n",
        "creates the final class predictions by using an MLP head for classification."
      ],
      "metadata": {
        "id": "toVYxLqEoldi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, transformer, pool = 'cls', channels = 3):\n",
        "        super().__init__()\n",
        "        image_size_h, image_size_w = pair(image_size)\n",
        "        assert image_size_h % patch_size == 0 and image_size_w % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        num_patches = (image_size_h // patch_size) * (image_size_w // patch_size)\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "            nn.LayerNorm(dim)\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "id": "eIXMn3oR0gAB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The follwing defines an efficient transformer model architecture called Linformer. It has the following properties:\n",
        "\n",
        "Input dimension: 128\n",
        "\n",
        "Sequence length: 49+1 (7x7 patches + 1 cls-token)\n",
        "\n",
        "Depth: 12 layers\n",
        "\n",
        "Number of attention heads: 8\n",
        "\n",
        "Key dimension: 64"
      ],
      "metadata": {
        "id": "SN_Qs9LUoSF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficient_transformer = Linformer(\n",
        "    dim=128,\n",
        "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
        "    depth=12,\n",
        "    heads=8,\n",
        "    k=64\n",
        ")"
      ],
      "metadata": {
        "id": "B2af_reH258K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " It checks whether a CUDA-compatible GPU is available. If so, it sets the device to “cuda” (for GPU acceleration). Otherwise, it defaults to “cpu” (for CPU-based computations). The selected device determines where tensor operations and model training/inference occur."
      ],
      "metadata": {
        "id": "T7c6LrQFpRsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "85w0F-0vC4QA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Vision Transformer (ViT) is initially initialized with certain settings by the model variable:\n",
        "\n",
        "With a 128x128 embedding dimension, it can handle 224x224 images with 32x32 patches.\n",
        "\n",
        "The model uses the supplied efficient_transformer to classify data into ten types.\n",
        "\n",
        "It uses three-channel pictures for operation, and it moves to the GPU or CPU depending on what is available."
      ],
      "metadata": {
        "id": "R6KRyA4KpXoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViT(\n",
        "    dim=128,\n",
        "    image_size=224,\n",
        "    patch_size=32,\n",
        "    num_classes=10,\n",
        "    transformer=efficient_transformer,\n",
        "    channels=3,\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "P5Nkk_1c3YAg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Size: The number of data samples processed in one iteration during training, set to 64.\n",
        "\n",
        "Epochs: The number of times the entire dataset is passed forward and backward through the model during training, set to 2.\n",
        "\n",
        "Learning Rate (lr): The rate at which the model parameters are updated during optimization, set to 3e-5 (0.00003).\n",
        "\n",
        "Gamma: The factor by which the learning rate is reduced after each epoch, set to 0.7, which reduces the learning rate by 30% after each epoch.\n",
        "\n",
        "Seed: The random seed used for initializing the random number generator, set to 42 for reproducibility of results."
      ],
      "metadata": {
        "id": "8HD0e5U4p9vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 2\n",
        "lr = 3e-5\n",
        "gamma = 0.7\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "eJTgKrT55sMU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function (criterion): It defines the loss function used for training the model. In this case, it's nn.CrossEntropyLoss(), commonly used for classification tasks.\n",
        "\n",
        "Optimizer (optimizer): It defines the optimization algorithm used to update the model parameters during training. Here, it's optim.Adam with a learning rate (lr) of 3e-5 (0.00003), optimizing the parameters of the model.\n",
        "\n",
        "Scheduler (scheduler): It defines a learning rate scheduler that adjusts the learning rate during training. The StepLR scheduler reduces the learning rate by a factor of gamma (0.7 in this case) every step_size (1 epoch) to improve convergence."
      ],
      "metadata": {
        "id": "O8pHJ7ziqJ_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "metadata": {
        "id": "IwegvyCO5hHH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code tells that Within the designated number of epochs, it loops over the batches of training data (from train_loader). It transfers the data and labels to the designated GPU device for every batch, computes the model's output, computes the loss using the established criterion (CrossEntropyLoss), updates the model's parameters via backpropagation using the optimizer (Adam), and computes the training accuracy. It calculates validation loss and accuracy by assessing the model's performance on the validation data (from test_loader) at the end of each epoch. Following every epoch, the training progress—which includes the loss, accuracy, validation accuracy, and validation loss—is printed."
      ],
      "metadata": {
        "id": "58dPhcawqnW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for data, label in train_loader:\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in test_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(test_loader)\n",
        "            epoch_val_loss += val_loss / len(test_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWMN0Fgn5oSO",
        "outputId": "82f89306-00ed-47d1-b1f2-f9e715a621ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 - loss : 1.9595 - acc: 0.2796 - val_loss : 1.7139 - val_acc: 0.3856\n",
            "\n",
            "Epoch : 2 - loss : 1.6393 - acc: 0.4171 - val_loss : 1.5789 - val_acc: 0.4370\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code illustrates about a trained deep learning model to visualize and predict a random test image. From the test loader dataset, a test image and its true label are first chosen at random. Next, it uses matplotlib to display the image, putting the true label in the title. The model is used to predict the label of the image after preprocessing it and transferring it to the designated GPU. This is done by setting the model to evaluation mode (model.eval()). Taking the class index from the model's output that has the highest probability will yield the projected label. In the end, it prints the predicted label as well as the genuine label for comparison."
      ],
      "metadata": {
        "id": "--92651HrPt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Selecting a random test image\n",
        "img, true_label = random.choice(list(test_loader.dataset))\n",
        "\n",
        "\n",
        "# For Displaying the image\n",
        "plt.imshow(img.permute(1, 2, 0))  # Adjust for matplotlib\n",
        "plt.title(f\"True Label: {classes[true_label]}\")\n",
        "plt.show()\n",
        "\n",
        "# Preprocessing and predict\n",
        "img = img.unsqueeze(0).to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(img)\n",
        "    predicted_label = output.argmax(dim=1).item()\n",
        "\n",
        "# Printing the actual and the predicted labels\n",
        "print(f\"Actual Label is: {classes[true_label]}\")\n",
        "print(f\"Model Predicted Label: {classes[predicted_label]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "6UgzRiw96I9C",
        "outputId": "fa3e40e5-869d-479e-c7f4-b6f87576c38c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Label is: dog\n",
            "Model Predicted Label: cat\n"
          ]
        }
      ]
    }
  ]
}